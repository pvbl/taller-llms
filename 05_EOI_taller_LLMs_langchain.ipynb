{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZPPL0rUi5ifL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70d3a59b-54a4-4f0d-f439-dc49b8149c08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.24-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: langchain-core in /usr/local/lib/python3.11/dist-packages (0.3.59)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.25 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.25)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.40)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.1.2)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.9.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.42)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (4.13.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (2.11.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core) (3.0.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.25->langchain-community) (0.3.8)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core) (0.4.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2025.4.26)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (0.16.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.3.1)\n",
            "Downloading langchain_community-0.3.24-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m68.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading pydantic_settings-2.9.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain-community\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-community-0.3.24 marshmallow-3.26.1 mypy-extensions-1.1.0 pydantic-settings-2.9.1 python-dotenv-1.1.0 typing-inspect-0.9.0\n",
            "Collecting langchain-huggingface\n",
            "  Downloading langchain_huggingface-0.2.0-py3-none-any.whl.metadata (941 bytes)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.59 in /usr/local/lib/python3.11/dist-packages (from langchain-huggingface) (0.3.59)\n",
            "Requirement already satisfied: tokenizers>=0.19.1 in /usr/local/lib/python3.11/dist-packages (from langchain-huggingface) (0.21.1)\n",
            "Requirement already satisfied: transformers>=4.39.0 in /usr/local/lib/python3.11/dist-packages (from langchain-huggingface) (4.51.3)\n",
            "Requirement already satisfied: sentence-transformers>=2.6.0 in /usr/local/lib/python3.11/dist-packages (from langchain-huggingface) (4.1.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.30.2 in /usr/local/lib/python3.11/dist-packages (from langchain-huggingface) (0.31.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.30.2->langchain-huggingface) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.30.2->langchain-huggingface) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.30.2->langchain-huggingface) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.30.2->langchain-huggingface) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.30.2->langchain-huggingface) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.30.2->langchain-huggingface) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.30.2->langchain-huggingface) (4.13.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.30.2->langchain-huggingface) (1.1.0)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.59->langchain-huggingface) (0.3.42)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.59->langchain-huggingface) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.59->langchain-huggingface) (1.33)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.59->langchain-huggingface) (2.11.4)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (1.15.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (11.2.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.39.0->langchain-huggingface) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.39.0->langchain-huggingface) (2024.11.6)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.39.0->langchain-huggingface) (0.5.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.59->langchain-huggingface) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.59->langchain-huggingface) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.59->langchain-huggingface) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.59->langchain-huggingface) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.59->langchain-huggingface) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.59->langchain-huggingface) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.59->langchain-huggingface) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.59->langchain-huggingface) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.30.2->langchain-huggingface) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.30.2->langchain-huggingface) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.30.2->langchain-huggingface) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.30.2->langchain-huggingface) (2025.4.26)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (1.3.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface) (3.6.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.59->langchain-huggingface) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.59->langchain-huggingface) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.59->langchain-huggingface) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (3.0.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.59->langchain-huggingface) (1.3.1)\n",
            "Downloading langchain_huggingface-0.2.0-py3-none-any.whl (27 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m96.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m78.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m46.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m65.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, langchain-huggingface\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed langchain-huggingface-0.2.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m680.9/680.9 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m59.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for google-search-results (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-0.3.16-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.58 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (0.3.59)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.68.2 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (1.78.1)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (0.9.0)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain-openai) (0.3.42)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain-openai) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain-openai) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain-openai) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain-openai) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain-openai) (4.13.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain-openai) (2.11.4)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (4.67.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2.32.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.68.2->langchain-openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.68.2->langchain-openai) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.68.2->langchain-openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.68.2->langchain-openai) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain-openai) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.58->langchain-openai) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.58->langchain-openai) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.58->langchain-openai) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.58->langchain-openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.58->langchain-openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.58->langchain-openai) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (2.4.0)\n",
            "Downloading langchain_openai-0.3.16-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: langchain-openai\n",
            "Successfully installed langchain-openai-0.3.16\n"
          ]
        }
      ],
      "source": [
        "!pip install -qU langchain --upgrade\n",
        "!pip install langchain-community langchain-core\n",
        "!pip install -U langchain-huggingface\n",
        "!pip install -qU huggingface_hub\n",
        "!pip install -qU openai\n",
        "!pip install -qU tiktoken\n",
        "!pip install -qU faiss-cpu\n",
        "!pip install -qU google-search-results\n",
        "!pip install -U langchain-openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate, HuggingFaceHub, LLMChain\n",
        "from langchain_openai import OpenAI, ChatOpenAI\n",
        "import openai\n",
        "import os\n",
        "from langchain.schema import Document\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.prompts.example_selector import SemanticSimilarityExampleSelector\n",
        "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
        "import datetime\n"
      ],
      "metadata": {
        "id": "HUNLbDqbzhXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPEN_AI_KEY_P')\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = userdata.get('HF_TOKEN')"
      ],
      "metadata": {
        "id": "DCLiRS530ils"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Básicos de interacción\n",
        "## OpenAI"
      ],
      "metadata": {
        "id": "WOCStf3z1dNf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "openai_dft = OpenAI(temperature=0.9)\n",
        "text = \"¿Cuál serie un nombre original para una compañía de palomitas?\"\n",
        "print(openai_dft(text))"
      ],
      "metadata": {
        "id": "LzZNBgMczhyd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b21f923-a7e8-49f8-91e0-d3b2a19e613c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\"CornTopia\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "m2 = OpenAI(model_name='gpt-4o-mini')\n",
        "text = \"¿Cuál serie un nombre original para una compañía de palomitas?\"\n",
        "print(m2(text))"
      ],
      "metadata": {
        "id": "gBNUqBWI3r4Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e2bb01a-c91e-4445-8861-b33e284a4209"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**  \n",
            "   \"PopArt: Palomitas con Estilo\"\n",
            "\n",
            "2. **¿Qué tipo de palomitas ofrecería \"PopArt\"?**  \n",
            "   Ofrecería una variedad de sabores gourmet, como trufa negra, caramelo salado, queso cheddar picante y palomitas de chocolate.\n",
            "\n",
            "3. **¿Qué aspecto visual tendría el empaque?**  \n",
            "   El empaque sería colorido y artístico, con ilustraciones vibrantes que representan los sabores únicos de cada tipo de palomita, creando una experiencia visual atractiva.\n",
            "\n",
            "4. **¿Cómo podría \"PopArt\" diferenciarse de otras marcas?**  \n",
            "   \"PopArt\" podría diferenciarse a través de colaboraciones con artistas locales para crear ediciones limitadas de sabores y empaques, así como ofreciendo talleres donde los clientes pueden crear su propia mezcla de palomitas personalizadas.\n",
            "\n",
            "5. **¿Qué tipo de eventos podría organizar \"PopArt\"?**  \n",
            "   Podría organizar noches de cine al aire libre con degustación de palomitas, festivales de sabores donde los asistentes pueden probar diferentes combinaciones y talleres creativos de arte y palomitas.\n",
            "\n",
            "6. **¿Cuál sería la misión de \"PopArt\"?**  \n",
            "   La misión de \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Templates para prompts"
      ],
      "metadata": {
        "id": "-auXnI-nzl0t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#prompt = PromptTemplate(\n",
        "#    input_variables=[\"product\"],\n",
        "#    template=\"What is a good name for a company that makes {product}?\",\n",
        "#)\n"
      ],
      "metadata": {
        "id": "2DHJiwrIzlC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# build prompt template for simple question-answering\n",
        "template = \"\"\"Pregunta: {question}\n",
        "\n",
        "Respuesta: \"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
        "\n",
        "llm_chain = LLMChain(\n",
        "    prompt=prompt,\n",
        "    llm=openai_dft\n",
        ")\n",
        "\n",
        "question = \"¿Qué equipo de fútbol ganó la Champions League en 2018?\"\n",
        "\n",
        "llm_chain = prompt | openai_dft\n",
        "llm_chain.invoke({\"question\": question})"
      ],
      "metadata": {
        "id": "cS5-3D7f30Ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c2327a78-d803-4aba-975a-e79dd713bf69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'El equipo que ganó la Champions League en 2018 fue el Real Madrid.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos hacer una cadena de preguntas y pasarlas todas a la vez"
      ],
      "metadata": {
        "id": "-BsfK4S3VUF9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "questions_lists = [\"¿Qué equipo de fútbol ganó la Champions League en 2018?\",\n",
        "             \"Si mido 190 en centimetros, ¿cuán alto soy en inchas?\",\n",
        "             \"¿Quién fue el primer hombre en llegar a la luna?\",\n",
        "             \"¿Cuántos ojos tiene una araña?\"]\n",
        "\n",
        "res = llm_chain.invoke(questions_lists)\n",
        "res"
      ],
      "metadata": {
        "id": "qoy3ytT0AZiN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "11b5f972-1192-456e-fb1a-70a87dfda925"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n1) El equipo que ganó la Champions League en 2018 fue el Real Madrid.\\n2) Si mides 190 centímetros, eres aproximadamente 74.8 pulgadas de altura.\\n3) El primer hombre en llegar a la luna fue Neil Armstrong.\\n4) Una araña tiene ocho ojos en promedio. Algunas especies pueden tener menos ojos y otras pueden tener más. '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "O meter todas las preguntas al mismo tiempo"
      ],
      "metadata": {
        "id": "JGUFa-NYVRbD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## HuggingFace"
      ],
      "metadata": {
        "id": "OW-i1j3e1hxE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import HuggingFacePipeline\n",
        "\"\"\"\n",
        "llm = HuggingFacePipeline.from_model_id(\n",
        "    model_id=\"google/gemma-3-1b-it\",\n",
        "    task=\"text-generation\",\n",
        "    pipeline_kwargs={\n",
        "        \"max_new_tokens\": 100,\n",
        "        \"top_k\": 50,\n",
        "        \"temperature\": 0.1,\n",
        "    },\n",
        ")\n",
        "llm.invoke(\"Las galletas son...\")\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "1JBX3Of2YBrO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "01b9c9d9-343a-4758-91e1-4ec1b6cf1181"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nllm = HuggingFacePipeline.from_model_id(\\n    model_id=\"google/gemma-3-1b-it\",\\n    task=\"text-generation\",\\n    pipeline_kwargs={\\n        \"max_new_tokens\": 100,\\n        \"top_k\": 50,\\n        \"temperature\": 0.1,\\n    },\\n)\\nllm.invoke(\"Las galletas son...\")\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Xb8OB5HEYHoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chatbots"
      ],
      "metadata": {
        "id": "rSDHFZ54Oe50"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n"
      ],
      "metadata": {
        "id": "RmsUlm-YOf6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat = ChatOpenAI(temperature=.7)\n"
      ],
      "metadata": {
        "id": "CvU1XNMaOjp1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response_q = chat.invoke(\n",
        "    [\n",
        "        SystemMessage(content=\"Eres un chatbot divertido y simpático al que le paso comida y me recomienda un plato\"),\n",
        "        HumanMessage(content=\"Me gustan los huevos, ¿Qué me puedo hacer?\")\n",
        "    ]\n",
        ")\n",
        "print(response_q.content)"
      ],
      "metadata": {
        "id": "_Td-Q2KzOv4C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e11a36f0-5af9-4fee-b0e3-fa6fb7e20fbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "¡Hola! ¡Los huevos son una excelente elección! Puedes preparar unos deliciosos huevos revueltos con queso y espinacas. Simplemente bate los huevos, añade un poco de queso rallado y saltea las espinacas en una sartén con un poco de aceite de oliva. ¡Una combinación deliciosa y fácil de preparar! ¡Espero que disfrutes tu comida! ¡Buen provecho!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se puede pasar un historial de chat"
      ],
      "metadata": {
        "id": "3PDFByHWO5hk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = chat.invoke(\n",
        "    [\n",
        "        SystemMessage(content=\"Eres un chatbot divertido y simpático al que le paso comida y me recomienda un plato\"),\n",
        "        HumanMessage(content=\"Me gustan los huevos, ¿Qué me puedo hacer?\"),\n",
        "        AIMessage(content=response_q.content),\n",
        "        HumanMessage(content=\"y algo más sencillo?\")\n",
        "    ]\n",
        ")\n",
        "print(response.content)"
      ],
      "metadata": {
        "id": "uiSVxqtNO4of"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Memoria\n",
        "Para los chatbots es importante la memoria. LangChain permite eficientar este proceso"
      ],
      "metadata": {
        "id": "SFUwAKQLcNbw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ChatMessageHistory\n",
        "\n",
        "chat = ChatOpenAI(temperature=0)\n",
        "\n",
        "history = ChatMessageHistory()\n",
        "\n",
        "history.add_ai_message(\"Hola!\")\n",
        "\n",
        "history.add_user_message(\"¿Cuál es la capital de Francia?\")"
      ],
      "metadata": {
        "id": "oXi3P2q9dF3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history.messages\n"
      ],
      "metadata": {
        "id": "D2M9I3C_dJUg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0380703-4572-426b-864d-0f4170f9555e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[AIMessage(content='Hola!', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='¿Cuál es la capital de Francia?', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ai_response = chat(history.messages)\n",
        "ai_response"
      ],
      "metadata": {
        "id": "Lex3p4MYdKy2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "caf3a67c-004f-42f2-e2cb-4596f6d5e0b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-2cd0e463527a>:1: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  ai_response = chat(history.messages)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='La capital de Francia es París.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 23, 'total_tokens': 33, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BWPVvwbtK4SFfbVwwYlzB5TlX8tU4', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--b95a8f44-e93a-4c7b-b72b-4f1bbf1b3fe5-0', usage_metadata={'input_tokens': 23, 'output_tokens': 10, 'total_tokens': 33, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history.add_ai_message(ai_response.content)\n",
        "history.messages"
      ],
      "metadata": {
        "id": "ZveS0FSHdRuz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d4077fc-cf8e-4b54-b624-fe73148546d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[AIMessage(content='Hola!', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='¿Cuál es la capital de Francia?', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='La capital de Francia es París.', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embeddings"
      ],
      "metadata": {
        "id": "vBIxM4p4agIp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n"
      ],
      "metadata": {
        "id": "A56bfvkVahU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = OpenAIEmbeddings()\n"
      ],
      "metadata": {
        "id": "iJTyhydea0yY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb43ddb4-6286-4f84-a1b0-eaaac761635f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-73ad2f8e367a>:1: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
            "  embeddings = OpenAIEmbeddings()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Me encanta programar en Python para mi taller de NLP.\"\n"
      ],
      "metadata": {
        "id": "wj7lJ8RKaz22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "text_embedding = embeddings.embed_query(text)\n",
        "print (f\"Tamaño del embedding: {len(text_embedding)}\")\n",
        "print (f\"Ejemplo: {text_embedding[:5]}...\")"
      ],
      "metadata": {
        "id": "fhEqrFoCazKE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d84207f-ad7a-462e-de41-84fc034517b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tamaño del embedding: 1536\n",
            "Ejemplo: [-0.022517696355198298, -0.017700733878092838, 0.007983313059868076, -0.03296087358346386, -0.004929358678019859]...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_2 = \"Soy un amante de la programación en Python. Siempre que puedo me pico unas líneas de código\"\n",
        "\n",
        "text_embedding2 = embeddings.embed_query(text_2)"
      ],
      "metadata": {
        "id": "LTnGzbu4bbat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cosine_similarity([text_embedding],[text_embedding2])"
      ],
      "metadata": {
        "id": "x_SaDebBbu1E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fde078c7-527e-47e9-ec14-87293bdefbe8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.90155507]])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Documentos\n",
        "Existe el objeto tipo documento que permite interacturar, evaluar similitudes y otras cosas entre documentos ( se verá luego)"
      ],
      "metadata": {
        "id": "ypHIWYMBW0mS"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YriLzc9CW1eW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = Document(page_content=\"Este es un documento superchulo que habla de lo que mola estudiar NLP y hacer un taller con chatGPT.\",\n",
        "         metadata={\n",
        "             'my_document_id' : 1,\n",
        "             'my_document_source' : \"Propia\",\n",
        "             'my_document_create_time' : datetime.datetime.now().timestamp()\n",
        "         })\n",
        "doc"
      ],
      "metadata": {
        "id": "m4xo1jnQW44_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffb008cd-2afb-400d-d710-27866b4476fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={'my_document_id': 1, 'my_document_source': 'Propia', 'my_document_create_time': 1747064294.036574}, page_content='Este es un documento superchulo que habla de lo que mola estudiar NLP y hacer un taller con chatGPT.')"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrievers\n",
        "Permiten cargar múltiples documentos y gestionarlos de forma eficiente"
      ],
      "metadata": {
        "id": "wWQeaiHfWSkR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/gkamradt/langchain-tutorials/main/data/PaulGrahamEssays/worked.txt"
      ],
      "metadata": {
        "id": "QkGbjB99agbX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "653fa219-1547-43d9-dec1-a55bf5411202"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-13 06:35:21--  https://raw.githubusercontent.com/gkamradt/langchain-tutorials/main/data/PaulGrahamEssays/worked.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 75768 (74K) [text/plain]\n",
            "Saving to: ‘worked.txt’\n",
            "\n",
            "worked.txt          100%[===================>]  73.99K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2025-05-13 06:35:22 (54.1 MB/s) - ‘worked.txt’ saved [75768/75768]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "loader = TextLoader('worked.txt')\n",
        "documents = loader.load()"
      ],
      "metadata": {
        "id": "De3f7mghWRPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get your splitter ready\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
        "\n",
        "# Split your docs into texts\n",
        "texts = text_splitter.split_documents(documents)\n",
        "\n",
        "# Get embedding engine ready\n",
        "embeddings = OpenAIEmbeddings()\n",
        "\n",
        "# Embedd your texts\n",
        "db = FAISS.from_documents(texts, embeddings)"
      ],
      "metadata": {
        "id": "zOB19RGNZN6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Init your retriever. Asking for just 1 document back\n",
        "retriever = db.as_retriever()"
      ],
      "metadata": {
        "id": "5uEs73V2asgf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = retriever.get_relevant_documents(\"what types of things did the author want to build?\")\n"
      ],
      "metadata": {
        "id": "DrdM8i6TcAWw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61cd3f9d-085e-485c-845d-6d5a829e37e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-22-5ade702ebef5>:1: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  docs = retriever.get_relevant_documents(\"what types of things did the author want to build?\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\\n\".join([x.page_content[:200] for x in docs[:2]]))\n"
      ],
      "metadata": {
        "id": "OCeBwCZFcBVx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2b62ad9-9229-43e2-c949-f40aca3556b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "standards; what was the point? No one else wanted one either, so\n",
            "off they went. That was what happened to systems work.I wanted not just to build things, but to build things that would\n",
            "last.In this di\n",
            "\n",
            "infrastructure, and the two undergrads worked on the first two\n",
            "services (images and phone calls). But about halfway through the\n",
            "summer I realized I really didn't want to run a company  especially\n",
            "not\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Selectores de ejemplos y vectorStores\n",
        "Permite seleccionar entre múltiples ejemplos de una manera sencilla"
      ],
      "metadata": {
        "id": "Ux1KmUK6dPtk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "llm = OpenAI(model_name=\"gpt-4o-mini\")\n",
        "\n",
        "example_prompt = PromptTemplate(\n",
        "    input_variables=[\"input\", \"output\"],\n",
        "    template=\"Ejemplo Input: {input}\\nEjemplo Output: {output}\",\n",
        ")\n",
        "\n",
        "# Examples of locations that nouns are found\n",
        "examples = [\n",
        "    {\"input\": \"pirata\", \"output\": \"barco\"},\n",
        "    {\"input\": \"piloto\", \"output\": \"avión\"},\n",
        "    {\"input\": \"conductor\", \"output\": \"coche\"},\n",
        "    {\"input\": \"árbol\", \"output\": \"suelo\"},\n",
        "    {\"input\": \"pájaro\", \"output\": \"nido\"},\n",
        "]"
      ],
      "metadata": {
        "id": "3Op93aEcdRlt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SemanticSimilarityExampleSelector seleccionará ejemplos que son similares al input con similitud semántica\n",
        "\n",
        "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
        "    # Lista de ejemplos\n",
        "    examples,\n",
        "\n",
        "    # This is the embedding class used to produce embeddings which are used to measure semantic similarity.\n",
        "    # esto creará embeddings para medir la similitud semántica\n",
        "    OpenAIEmbeddings(),\n",
        "\n",
        "    # Esto es el vectorStore que guardará los embeddings y analizará la similitud sobre ellos\n",
        "    FAISS,\n",
        "\n",
        "    # Número de ejemplos a producir.\n",
        "    k=2\n",
        ")\n"
      ],
      "metadata": {
        "id": "BAGiPslHgy-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "similar_prompt = FewShotPromptTemplate(\n",
        "    # Lo que ayudará a seleccionar ejemplos\n",
        "    example_selector=example_selector,\n",
        "\n",
        "    # El prompt\n",
        "    example_prompt=example_prompt,\n",
        "\n",
        "    # personalizaciones que serán añadidas arriba y debajo del prompt\n",
        "    prefix=\"Dada la localización de un item, normalmente se encuentra en\",\n",
        "    suffix=\"Input: {noun}\\nOutput:\",\n",
        "\n",
        "    # Inputs del prompt\n",
        "    input_variables=[\"noun\"],\n",
        ")"
      ],
      "metadata": {
        "id": "1yvch5_Iji82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Se introduce el input\n",
        "my_noun = \"estudiante\"\n",
        "\n",
        "print(similar_prompt.format(noun=my_noun))"
      ],
      "metadata": {
        "id": "wkse0FWakAsP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcbbc4d0-53bd-4ea2-d5fb-d360819ea91e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dada la localización de un item, normalmente se encuentra en\n",
            "\n",
            "Ejemplo Input: piloto\n",
            "Ejemplo Output: avión\n",
            "\n",
            "Ejemplo Input: conductor\n",
            "Ejemplo Output: coche\n",
            "\n",
            "Input: estudiante\n",
            "Output:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm.invoke(similar_prompt.format(noun=my_noun))"
      ],
      "metadata": {
        "id": "vKMCJsBckvnS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "2f5f589d-329c-48b6-87f3-a20dd4147737"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' escuela\\n\\nInput: maestro\\nOutput: clase\\n\\nInput: médico\\nOutput: hospital\\n\\nInput: marinero\\nOutput: barco\\n\\nInput: agricultor\\nOutput: granja\\n\\nInput: fotógrafo\\nOutput: cámara\\n\\nInput: jugador\\nOutput: campo\\n\\nInput: pintor\\nOutput: lienzo\\n\\nInput: escritor\\nOutput: libro\\n\\nInput: arquitecto\\nOutput: edificio\\n\\nInput: científico\\nOutput: laboratorio\\n\\nInput: vendedor\\nOutput: tienda\\n\\nInput: mecánico\\nOutput: taller\\n\\nInput: chef\\nOutput: cocina\\n\\nInput: taxista\\nOutput: taxi\\n\\nInput: electricista\\nOutput: cable\\n\\nInput: periodista\\nOutput: periódico\\n\\nInput: arqueólogo\\nOutput: excavación\\n\\nInput: bailarín\\nOutput: escenario\\n\\nInput: programador\\nOutput: computadora\\n\\nInput: psicólogo\\nOutput: consulta\\n\\nInput: farmacéutico\\nOutput: farmacia\\n\\nInput: bombero\\nOutput: camión\\n\\nInput: abogado\\nOutput: tribunal\\n\\nInput: carpintero\\nOutput: madera\\n\\nInput: locutor\\nOutput: micrófono\\n\\nInput: enfermero\\nOutput: hospital\\n\\nInput: jardinero\\nOutput: jardín\\n\\nInput: diseñador\\nOutput: diseño\\n\\nInput: guía\\nOutput'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Output Parsers\n",
        "Permiten formatear el output del modelo de una manera sencilla. Hay dos conceptos:\n",
        "1. Format Instructions - Un prompt que le dice al LLM cómo formatear la respuesta en función del output deseado\n",
        "2. Parser - Método que extrae el texto del modelo en una estructura deseada (generalmente JSON)"
      ],
      "metadata": {
        "id": "_7izgVNAk3GT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
        "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n"
      ],
      "metadata": {
        "id": "gbYzs7ORk5Jp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(model_name=\"gpt-4o-mini\")\n",
        "# How you would like your reponse structured. This is basically a fancy prompt template\n",
        "response_schemas = [\n",
        "    ResponseSchema(name=\"bad_string\", description=\"Esta es un texto mal escrito\"),\n",
        "    ResponseSchema(name=\"good_string\", description=\"Esta es tu respuesta,una respuesta corregida\")\n",
        "]\n",
        "\n",
        "# How you would like to parse your output\n",
        "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
        "# See the prompt template you created for formatting\n",
        "format_instructions = output_parser.get_format_instructions()\n",
        "print (format_instructions)"
      ],
      "metadata": {
        "id": "BotGwJIzk5M_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1086a303-49bf-45a5-c425-f27ae4b797fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
            "\n",
            "```json\n",
            "{\n",
            "\t\"bad_string\": string  // Esta es un texto mal escrito\n",
            "\t\"good_string\": string  // Esta es tu respuesta,una respuesta corregida\n",
            "}\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"\n",
        "Se te da un texto con errores gramaticales de un usuario.\n",
        "Tienes que corregirlo y estar seguro de que todas las palabras están correctamente escritas\n",
        "\n",
        "{format_instructions}\n",
        "\n",
        "% USER INPUT:\n",
        "{user_input}\n",
        "\n",
        "YOUR RESPONSE:\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"user_input\"],\n",
        "    partial_variables={\"format_instructions\": format_instructions},\n",
        "    template=template\n",
        ")\n",
        "\n",
        "promptValue = prompt.format(user_input=\"bienbenido a madrid\")\n",
        "\n",
        "print(promptValue)"
      ],
      "metadata": {
        "id": "HnlKW6nf5ugT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c91882c5-668d-4850-c0fd-ea97d3f61d2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Se te da un texto con errores gramaticales de un usuario.\n",
            "Tienes que corregirlo y estar seguro de que todas las palabras están correctamente escritas\n",
            "\n",
            "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
            "\n",
            "```json\n",
            "{\n",
            "\t\"bad_string\": string  // Esta es un texto mal escrito\n",
            "\t\"good_string\": string  // Esta es tu respuesta,una respuesta corregida\n",
            "}\n",
            "```\n",
            "\n",
            "% USER INPUT:\n",
            "bienbenido a madrid\n",
            "\n",
            "YOUR RESPONSE:\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_output = llm(promptValue)\n",
        "llm_output"
      ],
      "metadata": {
        "id": "xu03zmRMRjfq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "463e7127-52c5-4a5c-f989-101746d44f3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'```json\\n{\\n\\t\"bad_string\": \"bienbenido a madrid\",\\n\\t\"good_string\": \"bienvenido a Madrid\"\\n}\\n```'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_parser.parse(llm_output)\n"
      ],
      "metadata": {
        "id": "iMKYkkcbRkCv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad10cd03-523e-4e13-92f5-0e8b3652a068"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'bad_string': 'bienbenido a madrid', 'good_string': 'bienvenido a Madrid'}"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Agentes\n",
        "Un agente toma una entrada y devuelve una respuesta correspondiente a una acción a realizar junto con una entrada de acción. Puede ver diferentes tipos de agentes (cuáles son mejores para diferentes casos de uso). [Tipos de agentes](https://python.langchain.com/en/latest/modules/agents/agents/agent_types.html)\n"
      ],
      "metadata": {
        "id": "Lo2aS8MKiURS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import load_tools\n",
        "from langchain.agents import initialize_agent\n",
        "import json\n",
        "\n",
        "llm = OpenAI(temperature=0)"
      ],
      "metadata": {
        "id": "3IYpbE1giXAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['SERPAPI_API_KEY'] = userdata.get('SERPAPI_API_KEY')\n",
        "\n"
      ],
      "metadata": {
        "id": "-M3QMpXHtsBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "toolkit = load_tools([\"serpapi\"], llm=llm, serpapi_api_key=os.environ['SERPAPI_API_KEY'])\n"
      ],
      "metadata": {
        "id": "aKpl6UextuO-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# zero-shot-react-description: This agent uses the ReAct framework to determine which tool to use based solely on the tool’s description. Any number of tools can be provided. This agent requires that a description is provided for each tool.\n",
        "agent = initialize_agent(toolkit, llm, agent=\"zero-shot-react-description\", verbose=True, return_intermediate_steps=True)\n"
      ],
      "metadata": {
        "id": "XZqnSZSOtvsL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6bc9791-afdd-430c-906d-be1044f033fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-58-2e9f57a9a5ae>:2: LangChainDeprecationWarning: LangChain agents will continue to be supported, but it is recommended for new use cases to be built with LangGraph. LangGraph offers a more flexible and full-featured framework for building agents, including support for tool-calling, persistence of state, and human-in-the-loop workflows. For details, refer to the `LangGraph documentation <https://langchain-ai.github.io/langgraph/>`_ as well as guides for `Migrating from AgentExecutor <https://python.langchain.com/docs/how_to/migrate_agent/>`_ and LangGraph's `Pre-built ReAct agent <https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/>`_.\n",
            "  agent = initialize_agent(toolkit, llm, agent=\"zero-shot-react-description\", verbose=True, return_intermediate_steps=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = agent({\"input\":\"cual es la primera película en la que debutó\"\n",
        "                    \"Antonio Banderas?\"})"
      ],
      "metadata": {
        "id": "f0ldtmxytyTV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5105c54b-be44-418a-d8c7-fcd1021ed06d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-59-3d01e7c5cc83>:1: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  response = agent({\"input\":\"cual es la primera película en la que debutó\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m I should use a search engine to find the answer.\n",
            "Action: Search\n",
            "Action Input: \"Antonio Banderas debut movie\"\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mLabyrinth of Passion (1982)\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I should double check the information to make sure it is accurate.\n",
            "Action: Search\n",
            "Action Input: \"Antonio Banderas debut movie\"\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mLabyrinth of Passion (1982)\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer.\n",
            "Final Answer: Labyrinth of Passion (1982)\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response[\"intermediate_steps\"])\n"
      ],
      "metadata": {
        "id": "UvUK9hbA-_ft",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d49a3750-bfc1-4f5c-8caa-0685e8b57c6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(AgentAction(tool='Search', tool_input='Antonio Banderas debut movie', log=' I should use a search engine to find the answer.\\nAction: Search\\nAction Input: \"Antonio Banderas debut movie\"'), 'Labyrinth of Passion (1982)'), (AgentAction(tool='Search', tool_input='Antonio Banderas debut movie', log=' I should double check the information to make sure it is accurate.\\nAction: Search\\nAction Input: \"Antonio Banderas debut movie\"'), 'Labyrinth of Passion (1982)')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Referencias\n",
        "- https://colab.research.google.com/github/pinecone-io/examples/blob/master/generation/langchain/handbook/00-langchain-intro.ipynb\n",
        "- https://github.com/gkamradt/langchain-tutorials/blob/main/LangChain%20Cookbook.ipynb"
      ],
      "metadata": {
        "id": "OYTXMZdqgzaX"
      }
    }
  ]
}